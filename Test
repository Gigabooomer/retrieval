import os
from langchain_core.runnables import RemoteRunnable
from langchain_core.prompts import PromptTemplate

# Configuration
LLM_API = "your_llm_endpoint_here"  # Replace with your LLM API URL
TEMPLATE_FILE = "template.txt"  # Replace with actual template file path
SCHEMA_FILE = "schema.txt"  # Replace with actual schema file path

# Load LLM
llm = RemoteRunnable(LLM_API)

# Load the prompt template
with open(TEMPLATE_FILE, "r") as f:
    template = f.read()

prompt_ = PromptTemplate.from_template(template)

# Load schema
with open(SCHEMA_FILE, "r") as f:
    schema = f.read()

# Context Tracking
conversation_history = []

print("ðŸ”¹ Chatbot is running... (Type 'exit' to stop)")

while True:
    question = input("Enter question: ").strip()

    if question.lower() == "exit":
        print("ðŸ”¹ Exiting chatbot. Goodbye!")
        break

    if not question:
        print("âš ï¸ Please enter a valid question.")
        continue

    # Maintain conversation history (limit length to avoid context overload)
    conversation_history.append(f"User: {question}")
    if len(conversation_history) > 10:  # Limit to last 10 exchanges
        conversation_history.pop(0)

    # Format prompt with schema and conversation history
    context = "\n".join(conversation_history)
    prompt = prompt_.format(schema=schema, question=question, context=context)

    # Generate response
    response = ""
    print("ðŸ¤– AI Response:")
    for chunk in llm.stream(prompt):
        print(chunk, end="", flush=True)
        response += chunk
    print()

    # Store AI response in conversation history
    conversation_history.append(f"AI: {response}")
